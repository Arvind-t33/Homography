{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Images are derived from a public image set, that can be used for non-commercial purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt 1: \n",
    "I have two images of the same object, with some offset in angle and position. Stitch them into one panorama. I want to see the entire process of analysis and finding the points of interest in the images, aligning them, and stitching them together. You can only use cv2 for very basic tasks (read, write, decode etc) and perspectiveTransform (due to its complexity). Make this interactive on the python notebook by allowing users to upload their images using ipywidgets and display the final panorama. I am interested in the Harris and SIFT features and how they can be used to align the images. Since I want to understand the process, please provide a detailed explanation of each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e54e3a895d41b48d24c52fbb97606b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n<style>\\n    .widget-upload {\\n        margin: 10px;\\n    }\\n    .widget-button {\\n        backg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0070d68b8d374d8bb85d6fa0ee9b648c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(FileUpload(value=(), accept='image/*', description='Upload'), FileUpload(value=(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "# from skimage.transform import ProjectiveTransform\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2 \n",
    "from scipy.signal import convolve2d\n",
    "from scipy.ndimage import rank_filter\n",
    "from scipy.stats import norm\n",
    "from ipywidgets import FileUpload, Button, HBox, VBox, Output, HTML, Layout\n",
    "\n",
    "# ------------------------------------------------ Helper Functions ------------------------------------------------\n",
    "\n",
    "\n",
    "def dist2(x, c):\n",
    "    \"\"\"\n",
    "    Calculates squared distance between two sets of points.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: numpy.ndarray\n",
    "        Data of shape `(ndata, dimx)`\n",
    "    c: numpy.ndarray\n",
    "        Centers of shape `(ncenters, dimc)`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    n2: numpy.ndarray\n",
    "        Squared distances between each pair of data from x and c, of shape\n",
    "        `(ndata, ncenters)`\n",
    "    \"\"\"\n",
    "    assert x.shape[1] == c.shape[1], \\\n",
    "        'Data dimension does not match dimension of centers'\n",
    "\n",
    "    x = np.expand_dims(x, axis=0)  # new shape will be `(1, ndata, dimx)`\n",
    "    c = np.expand_dims(c, axis=1)  # new shape will be `(ncenters, 1, dimc)`\n",
    "\n",
    "    # We will now use broadcasting to easily calculate pairwise distances\n",
    "    n2 = np.sum((x - c) ** 2, axis=-1)\n",
    "\n",
    "    return n2\n",
    "\n",
    "\n",
    "def gen_dgauss(sigma):\n",
    "    \"\"\"\n",
    "    Generates the horizontally and vertically differentiated Gaussian filter\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma: float\n",
    "        Standard deviation of the Gaussian distribution\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Gx: numpy.ndarray\n",
    "        First degree derivative of the Gaussian filter across rows\n",
    "    Gy: numpy.ndarray\n",
    "        First degree derivative of the Gaussian filter across columns\n",
    "    \"\"\"\n",
    "    # Gaussian filter size\n",
    "    f_wid = 4 * np.floor(sigma)\n",
    "    # 1D Gaussian\n",
    "    G = norm.pdf(np.arange(-f_wid, f_wid + 1), loc=0, scale=sigma).reshape(-1, 1)\n",
    "    # 2D Gaussian\n",
    "    G = G.T * G\n",
    "    # Gradient of Gaussian\n",
    "    Gx, Gy = np.gradient(G)\n",
    "    # Normalize Gx and Gy\n",
    "    Gx = Gx * 2 / np.abs(Gx).sum()\n",
    "    Gy = Gy * 2 / np.abs(Gy).sum()\n",
    "\n",
    "    return Gx, Gy\n",
    "\n",
    "def rgb2gray(I):\n",
    "    \"\"\"\n",
    "    Convert an RGB image to grayscale.\n",
    "\n",
    "    Parameters:\n",
    "    I (numpy.ndarray): Input image array in RGB format.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Grayscale image array.\n",
    "    \"\"\"\n",
    "    # Convert numpy array to PIL Image\n",
    "    image = Image.fromarray(I.astype('uint8'), 'RGB')\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = image.convert('L')\n",
    "    # Convert back to numpy array\n",
    "    I_gray = np.array(gray_image)\n",
    "    return I_gray\n",
    "\n",
    "\n",
    "def find_sift(I, circles, enlarge_factor=1.5):\n",
    "    \"\"\"\n",
    "    Compute non-rotation-invariant SIFT descriptors of a set of circles\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    I: numpy.ndarray\n",
    "        Image\n",
    "    circles: numpy.ndarray\n",
    "        An array of shape `(ncircles, 3)` where ncircles is the number of\n",
    "        circles, and each circle is defined by (x, y, r), where r is the radius\n",
    "        of the cirlce\n",
    "    enlarge_factor: float\n",
    "        Factor which indicates by how much to enlarge the radius of the circle\n",
    "        before computing the descriptor (a factor of 1.5 or large is usually\n",
    "        necessary for best performance)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sift_arr: numpy.ndarray\n",
    "        Array of SIFT descriptors of shape `(ncircles, 128)`\n",
    "    \"\"\"\n",
    "    assert circles.ndim == 2 and circles.shape[1] == 3, \\\n",
    "        'Use circles array (keypoints array) of correct shape'\n",
    "\n",
    "    I = I.astype(np.float64)\n",
    "    if I.ndim == 3:\n",
    "        I = rgb2gray(I)  # Convert to grayscale if image is RGB\n",
    "\n",
    "    NUM_ANGLES = 8\n",
    "    NUM_BINS = 4\n",
    "    NUM_SAMPLES = NUM_BINS * NUM_BINS\n",
    "    ALPHA = 9\n",
    "    SIGMA_EDGE = 1\n",
    "\n",
    "    ANGLE_STEP = 2 * np.pi / NUM_ANGLES\n",
    "    angles = np.arange(0, 2 * np.pi, ANGLE_STEP)\n",
    "\n",
    "    height, width = I.shape[:2]\n",
    "    num_pts = circles.shape[0]\n",
    "\n",
    "    sift_arr = np.zeros((num_pts, NUM_SAMPLES * NUM_ANGLES))\n",
    "\n",
    "    Gx, Gy = gen_dgauss(SIGMA_EDGE)  # Generate Gaussian derivatives\n",
    "\n",
    "    Ix = convolve2d(I, Gx, 'same')  # Convolve with Gx\n",
    "    Iy = convolve2d(I, Gy, 'same')  # Convolve with Gy\n",
    "    I_mag = np.sqrt(Ix ** 2 + Iy ** 2)  # Magnitude of gradient\n",
    "    I_theta = np.arctan2(Ix, Iy + 1e-12)  # Orientation of gradient\n",
    "\n",
    "    interval = np.arange(-1 + 1/NUM_BINS, 1 + 1/NUM_BINS, 2/NUM_BINS)\n",
    "    gridx, gridy = np.meshgrid(interval, interval)\n",
    "    gridx = gridx.reshape((1, -1))\n",
    "    gridy = gridy.reshape((1, -1))\n",
    "\n",
    "    I_orientation = np.zeros((height, width, NUM_ANGLES))\n",
    "\n",
    "    for i in range(NUM_ANGLES):\n",
    "        tmp = np.cos(I_theta - angles[i]) ** ALPHA\n",
    "        tmp = tmp * (tmp > 0)  # Retain positive values only\n",
    "\n",
    "        I_orientation[:, :, i] = tmp * I_mag  # Weighted magnitude\n",
    "\n",
    "    for i in range(num_pts):\n",
    "        cx, cy = circles[i, :2]\n",
    "        r = circles[i, 2]\n",
    "\n",
    "        gridx_t = gridx * r + cx\n",
    "        gridy_t = gridy * r + cy\n",
    "        grid_res = 2.0 / NUM_BINS * r\n",
    "\n",
    "        x_lo = np.floor(np.max([cx - r - grid_res / 2, 0])).astype(np.int32)\n",
    "        x_hi = np.ceil(np.min([cx + r + grid_res / 2, width])).astype(np.int32)\n",
    "        y_lo = np.floor(np.max([cy - r - grid_res / 2, 0])).astype(np.int32)\n",
    "        y_hi = np.ceil(np.min([cy + r + grid_res / 2, height])).astype(np.int32)\n",
    "\n",
    "        grid_px, grid_py = np.meshgrid(\n",
    "            np.arange(x_lo, x_hi, 1),\n",
    "            np.arange(y_lo, y_hi, 1))\n",
    "        grid_px = grid_px.reshape((-1, 1))\n",
    "        grid_py = grid_py.reshape((-1, 1))\n",
    "\n",
    "        dist_px = np.abs(grid_px - gridx_t)\n",
    "        dist_py = np.abs(grid_py - gridy_t)\n",
    "\n",
    "        weight_x = dist_px / (grid_res + 1e-12)\n",
    "        weight_x = (1 - weight_x) * (weight_x <= 1)  # Weight based on distance\n",
    "        weight_y = dist_py / (grid_res + 1e-12)\n",
    "        weight_y = (1 - weight_y) * (weight_y <= 1)  # Weight based on distance\n",
    "        weights = weight_x * weight_y  # Combined weights\n",
    "\n",
    "        curr_sift = np.zeros((NUM_ANGLES, NUM_SAMPLES))\n",
    "        for j in range(NUM_ANGLES):\n",
    "            tmp = I_orientation[y_lo:y_hi, x_lo:x_hi, j].reshape((-1, 1))\n",
    "            curr_sift[j, :] = (tmp * weights).sum(axis=0)  # Weighted sum\n",
    "        sift_arr[i, :] = curr_sift.flatten()\n",
    "\n",
    "    tmp = np.sqrt(np.sum(sift_arr ** 2, axis=-1))\n",
    "    if np.sum(tmp > 1) > 0:\n",
    "        sift_arr_norm = sift_arr[tmp > 1, :]\n",
    "        sift_arr_norm /= tmp[tmp > 1].reshape(-1, 1)  # Normalize\n",
    "\n",
    "        sift_arr_norm = np.clip(sift_arr_norm, sift_arr_norm.min(), 0.2)  # Clip values\n",
    "\n",
    "        sift_arr_norm /= np.sqrt(\n",
    "            np.sum(sift_arr_norm ** 2, axis=-1, keepdims=True))  # Renormalize\n",
    "\n",
    "        sift_arr[tmp > 1, :] = sift_arr_norm\n",
    "\n",
    "    return sift_arr\n",
    "\n",
    "\n",
    "def harris(im, sigma, thresh=None, radius=None):\n",
    "    \"\"\"\n",
    "    Harris corner detector\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im: numpy.ndarray\n",
    "        Image to be processed\n",
    "    sigma: float\n",
    "        Standard deviation of smoothing Gaussian\n",
    "    thresh: float (optional)\n",
    "    radius: float (optional)\n",
    "        Radius of region considered in non-maximal suppression\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cim: numpy.ndarray\n",
    "        Binary image marking corners\n",
    "    r: numpy.ndarray\n",
    "        Row coordinates of corner points. Returned only if none of `thresh` and\n",
    "        `radius` are None.\n",
    "    c: numpy.ndarray\n",
    "        Column coordinates of corner points. Returned only if none of `thresh`\n",
    "        and `radius` are None.\n",
    "    \"\"\"\n",
    "    if im.ndim == 3:\n",
    "        im = rgb2gray(im)  # Convert to grayscale if image is RGB\n",
    "\n",
    "    dx = np.tile([[-1, 0, 1]], [3, 1])\n",
    "    dy = dx.T  # Transpose to get vertical Sobel filter\n",
    "\n",
    "    Ix = convolve2d(im, dx, 'same')  # Convolve with horizontal Sobel filter\n",
    "    Iy = convolve2d(im, dy, 'same')  # Convolve with vertical Sobel filter\n",
    "\n",
    "    f_wid = np.round(3 * np.floor(sigma))  # Gaussian filter size\n",
    "    G = norm.pdf(np.arange(-f_wid, f_wid + 1), loc=0, scale=sigma).reshape(-1, 1)\n",
    "    G = G.T * G  # Create 2D Gaussian filter\n",
    "    G /= G.sum()  # Normalize Gaussian filter\n",
    "\n",
    "    Ix2 = convolve2d(Ix ** 2, G, 'same')  # Smooth Ix^2 with Gaussian\n",
    "    Iy2 = convolve2d(Iy ** 2, G, 'same')  # Smooth Iy^2 with Gaussian\n",
    "    Ixy = convolve2d(Ix * Iy, G, 'same')  # Smooth Ix*Iy with Gaussian\n",
    "\n",
    "    cim = (Ix2 * Iy2 - Ixy ** 2) / (Ix2 + Iy2 + 1e-12)  # Harris corner response\n",
    "\n",
    "    if thresh is None or radius is None:\n",
    "        return cim\n",
    "    else:\n",
    "        size = int(2 * radius + 1)  # Size for rank filter\n",
    "        mx = rank_filter(cim, -1, size=size)  # Apply rank filter to find local maxima\n",
    "        cim = (cim == mx) & (cim > thresh)  # Threshold and find local maxima\n",
    "\n",
    "        r, c = cim.nonzero()  # Get coordinates of corner points\n",
    "\n",
    "        return cim, r, c\n",
    "    \n",
    "def draw_corners(img, r, c, img_name):\n",
    "    \"\"\"\n",
    "    Draws rectangles around detected corner points on an image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img: numpy.ndarray\n",
    "        The image on which to draw the corners.\n",
    "    r: numpy.ndarray\n",
    "        Row coordinates of the corner points.\n",
    "    c: numpy.ndarray\n",
    "        Column coordinates of the corner points.\n",
    "    img_name: str\n",
    "        The name of the image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        The function saves the image with rectangles drawn around the corner points.\n",
    "    \"\"\"\n",
    "    # Create a copy of the image to draw on\n",
    "    img_copy = img.copy()\n",
    "\n",
    "    # Create a figure and axis for plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    # Loop through each corner point\n",
    "    for i in range(0, len(r)):\n",
    "        # Create a rectangle patch at each corner point\n",
    "        rect = patches.Rectangle((c[i], r[i]), 10, 10, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        # Add the rectangle patch to the axis\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # Display/save the image with the corner points highlighted\n",
    "    ax.imshow(img_copy, cmap='gray')\n",
    "    plt.savefig(f'./data/out/{img_name}_corners.jpg')\n",
    "    plt.close(fig)\n",
    "\n",
    "def filter_descriptors_by_dist(distances, thresh):\n",
    "    \"\"\"\n",
    "    Filters descriptor pairs based on a distance threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distances : numpy.ndarray\n",
    "        A 2D array where each element represents the distance between a pair of descriptors.\n",
    "    thresh : float\n",
    "        The distance threshold. Only descriptor pairs with distances below this threshold will be considered.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    filtered_matches : numpy.ndarray\n",
    "        A 2D array where each row contains the indices of the closest descriptor pairs that are below the threshold.\n",
    "        The first column contains the indices of the closest descriptors, and the second column contains the indices\n",
    "        of the original descriptors that are below the threshold.\n",
    "    \"\"\"\n",
    "    # Find the indices of the pairs that are below the threshold\n",
    "    indices_below_thresh = np.unique(np.where(distances < thresh)[0])\n",
    "    distances_below_thresh = distances[indices_below_thresh]\n",
    "    \n",
    "    # Find the closest descriptors by sorting the distances and taking the first element\n",
    "    closest_descs = np.argsort(distances_below_thresh, axis=1)[:, 0]\n",
    "\n",
    "    # Reshape the arrays to be column vectors\n",
    "    closest_descs = closest_descs.reshape(-1, 1)\n",
    "    indices_below_thresh = indices_below_thresh.reshape(-1, 1)\n",
    "    \n",
    "    # Concatenate the closest descriptors and their corresponding indices\n",
    "    filtered_matches = np.concatenate((closest_descs, indices_below_thresh), axis=1)\n",
    "    \n",
    "    return filtered_matches\n",
    "\n",
    "def draw_matches(img1, img2, r1, c1, r2, c2, matches):\n",
    "    \"\"\"\n",
    "    Draws lines matching the corners between two images using the filtered matches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img1 : numpy.ndarray\n",
    "        The first image.\n",
    "    img2 : numpy.ndarray\n",
    "        The second image.\n",
    "    r1 : numpy.ndarray\n",
    "        Row coordinates of the corner points in the first image.\n",
    "    c1 : numpy.ndarray\n",
    "        Column coordinates of the corner points in the first image.\n",
    "    r2 : numpy.ndarray\n",
    "        Row coordinates of the corner points in the second image.\n",
    "    c2 : numpy.ndarray\n",
    "        Column coordinates of the corner points in the second image.\n",
    "    matches : numpy.ndarray\n",
    "        A 2D array where each row contains the indices of matching corner points between the two images.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        The function displays/saves the two images side by side with lines connecting the matching corners.\n",
    "    \"\"\"\n",
    "    img1_copy = img1.copy()\n",
    "    # Create copies of the images to draw on\n",
    "    img1_copy = img1.copy()\n",
    "    img2_copy = img2.copy()\n",
    "\n",
    "    # Create a figure and axis for plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    # Display the two images side by side\n",
    "    ax.imshow(np.hstack((img1_copy, img2_copy)), cmap='gray')\n",
    "\n",
    "    # Loop through each match\n",
    "    for i in range(0, matches.shape[0]):\n",
    "        # Get coordinates of the matching point in the first image\n",
    "        x1 = c1[matches[i, 0]]\n",
    "        y1 = r1[matches[i, 0]]\n",
    "        \n",
    "        # Get coordinates of the matching point in the second image\n",
    "        # Offset x2 by the width of the first image to align side by side\n",
    "        x2 = c2[matches[i, 1]] + img1.shape[1]\n",
    "        y2 = r2[matches[i, 1]]\n",
    "        \n",
    "        # Draw a red line connecting the matching points\n",
    "        ax.plot([x1, x2], [y1, y2], 'r-')\n",
    "        \n",
    "    # Save the image\n",
    "    fig.savefig('./data/out/matches.jpg')\n",
    "\n",
    "def normalize_points(points):\n",
    "    \"\"\"\n",
    "    Normalize a set of points so that the centroid is at the origin and the average distance to the origin is sqrt(2).\n",
    "    \n",
    "    Parameters:\n",
    "    points (numpy.ndarray): Array of points of shape (N, 2).\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Normalized points.\n",
    "    numpy.ndarray: Normalization matrix.\n",
    "    \"\"\"\n",
    "    centroid = np.mean(points, axis=0)\n",
    "\n",
    "    # Center the points around the origin\n",
    "    centered_points = points - centroid\n",
    "\n",
    "    # Calculate the average distance of the points from the origin\n",
    "    avg_dist = np.mean(np.sqrt(np.sum(centered_points**2, axis=1)))\n",
    "\n",
    "    # Calculate the scaling factor\n",
    "    scale = np.sqrt(2) / avg_dist\n",
    "\n",
    "    # Create the normalization transformation matrix\n",
    "    T = np.array([\n",
    "        [scale, 0, -scale * centroid[0]],\n",
    "        [0, scale, -scale * centroid[1]],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    # Apply the normalization transformation to the points\n",
    "    normalized_points = (T @ np.vstack((points.T, np.ones((1, points.shape[0]))))).T\n",
    "\n",
    "    # Return the normalized points and the transformation matrix\n",
    "    return normalized_points[:, :2], T\n",
    "\n",
    "def compute_homography(points1, points2):\n",
    "    \"\"\"\n",
    "    Compute the homography matrix from points1 to points2 using the DLT algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    points1 (numpy.ndarray): Array of points in the first image of shape (N, 2).\n",
    "    points2 (numpy.ndarray): Array of points in the second image of shape (N, 2).\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Homography matrix of shape (3, 3).\n",
    "    \"\"\"\n",
    "    # Normalize the points\n",
    "    points1_normalized, T1 = normalize_points(points1)\n",
    "    points2_normalized, T2 = normalize_points(points2)\n",
    "    \n",
    "    # Set up the linear system of equations\n",
    "    N = points1.shape[0]\n",
    "    A = []\n",
    "    for i in range(N):\n",
    "        x1, y1 = points1_normalized[i]\n",
    "        x2, y2 = points2_normalized[i]\n",
    "        A.append([-x1, -y1, -1, 0, 0, 0, x2*x1, x2*y1, x2])\n",
    "        A.append([0, 0, 0, -x1, -y1, -1, y2*x1, y2*y1, y2])\n",
    "    A = np.array(A)\n",
    "    \n",
    "    # Solve the system using SVD\n",
    "    U, S, Vt = np.linalg.svd(A)\n",
    "    H_normalized = Vt[-1].reshape(3, 3)\n",
    "    \n",
    "    # Denormalize the homography matrix\n",
    "    H = np.linalg.inv(T2) @ H_normalized @ T1\n",
    "    \n",
    "    # Normalize so that H[2, 2] is 1\n",
    "    H /= H[2, 2]\n",
    "    \n",
    "    return H\n",
    "\n",
    "def RANSAC(filtered_matches, eps, num_loops, r1, c1, r2, c2):\n",
    "    \"\"\"\n",
    "    Performs the RANSAC algorithm to find the best homography matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filtered_matches : numpy.ndarray\n",
    "        A 2D array where each row contains the indices of matching corner points between two images.\n",
    "    eps : float\n",
    "        The distance threshold to determine inliers.\n",
    "    num_loops : int\n",
    "        The number of iterations to run the RANSAC algorithm.\n",
    "    r1 : numpy.ndarray\n",
    "        Row coordinates of the corner points in the first image.\n",
    "    c1 : numpy.ndarray\n",
    "        Column coordinates of the corner points in the first image.\n",
    "    r2 : numpy.ndarray\n",
    "        Row coordinates of the corner points in the second image.\n",
    "    c2 : numpy.ndarray\n",
    "        Column coordinates of the corner points in the second image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_H : numpy.ndarray\n",
    "        The best homography matrix found.\n",
    "    max_inliers : int\n",
    "        The maximum number of inliers found.\n",
    "    best_residual : float\n",
    "        The mean residual of the inliers.\n",
    "    inlier_matches : numpy.ndarray\n",
    "        The inlier matches corresponding to the best homography matrix.\n",
    "    \"\"\"\n",
    "    max_inliers = 0\n",
    "    best_residual = float('inf')\n",
    "    best_H = None\n",
    "    inlier_matches = None\n",
    "\n",
    "    for i in range(0, num_loops):\n",
    "        # randomly select 4 points\n",
    "        indices = np.random.choice(filtered_matches.shape[0], 4, replace=False)\n",
    "        selected_matches = filtered_matches[indices]\n",
    "\n",
    "        # prepare points for cv2.findHomography\n",
    "        points1 = np.float32([c1[selected_matches[:, 0]], r1[selected_matches[:, 0]]]).T\n",
    "        points2 = np.float32([c2[selected_matches[:, 1]], r2[selected_matches[:, 1]]]).T\n",
    "\n",
    "        # compute homography\n",
    "        H = compute_homography(points1, points2)\n",
    "\n",
    "        # compute predicted coordinates\n",
    "        pred_coords = np.dot(H, np.vstack((c1[filtered_matches[:, 0]], r1[filtered_matches[:, 0]], np.ones(filtered_matches.shape[0]))))\n",
    "        pred_coords = pred_coords / pred_coords[2]\n",
    "\n",
    "        # calculate distance\n",
    "        dist = np.sqrt(np.sum((np.vstack((c2[filtered_matches[:, 1]], r2[filtered_matches[:, 1]])) - pred_coords[:2]) ** 2, axis=0))\n",
    "\n",
    "        # find indices where distance is less than threshold\n",
    "        inliers = np.where(dist < eps)[0]\n",
    "\n",
    "        if len(inliers) > max_inliers:\n",
    "            max_inliers = len(inliers)\n",
    "            best_residual = np.mean(dist[inliers])\n",
    "            best_H = H\n",
    "            inlier_matches = filtered_matches[inliers]\n",
    "\n",
    "    return best_H, max_inliers, best_residual, inlier_matches\n",
    "\n",
    "def draw_inliers(img1, img2, r1, c1, r2, c2, inlier_matches):\n",
    "    \"\"\"\n",
    "    Draws lines connecting inlier matches between two images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img1 : numpy.ndarray\n",
    "        The first image.\n",
    "    img2 : numpy.ndarray\n",
    "        The second image.\n",
    "    r1 : numpy.ndarray\n",
    "        Row coordinates of the corner points in the first image.\n",
    "    c1 : numpy.ndarray\n",
    "        Column coordinates of the corner points in the first image.\n",
    "    r2 : numpy.ndarray\n",
    "        Row coordinates of the corner points in the second image.\n",
    "    c2 : numpy.ndarray\n",
    "        Column coordinates of the corner points in the second image.\n",
    "    inlier_matches : numpy.ndarray\n",
    "        A 2D array where each row contains the indices of inlier matches between the two images.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        The function displays/saves the two images side by side with lines connecting the inlier matches.\n",
    "    \"\"\"\n",
    "    # Create copies of the images to draw on\n",
    "    img1_copy = img1.copy()\n",
    "    img2_copy = img2.copy()\n",
    "\n",
    "    # Create a figure and axis for plotting\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "    # Concatenate the images side by side\n",
    "    combined_img = np.hstack((img1_copy, img2_copy))\n",
    "\n",
    "    # Display the combined image\n",
    "    ax.imshow(combined_img, cmap='gray')\n",
    "\n",
    "    # Loop through each inlier match\n",
    "    for i in range(inlier_matches.shape[0]):\n",
    "        # Get coordinates of the matching point in the first image\n",
    "        x1 = c1[inlier_matches[i, 0]]\n",
    "        y1 = r1[inlier_matches[i, 0]]\n",
    "\n",
    "        # Get coordinates of the matching point in the second image\n",
    "        # Offset x2 by the width of the first image to align side by side\n",
    "        x2 = c2[inlier_matches[i, 1]] + img1.shape[1]\n",
    "        y2 = r2[inlier_matches[i, 1]]\n",
    "\n",
    "        # Draw a line connecting the matching points\n",
    "        ax.plot([x1, x2], [y1, y2], '-', linewidth=1)\n",
    "\n",
    "    # Save the image\n",
    "    fig.savefig('./data/out/inliers.jpg')\n",
    "\n",
    "def warp_perspective_manual(image, H, output_shape):\n",
    "    \"\"\"\n",
    "    Apply a perspective warp to an image using a homography matrix.\n",
    "\n",
    "    Parameters:\n",
    "    image (numpy.ndarray): Input image.\n",
    "    H (numpy.ndarray): Homography matrix.\n",
    "    output_shape (tuple): Shape of the output image (height, width).\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Warped image.\n",
    "    \"\"\"\n",
    "    # Inverse homography matrix\n",
    "    H_inv = np.linalg.inv(H)\n",
    "    \n",
    "    # Check if the image is grayscale or color\n",
    "    if image.ndim == 2:\n",
    "        # Grayscale image\n",
    "        warped_image = np.zeros((output_shape[0], output_shape[1]), dtype=image.dtype)\n",
    "    else:\n",
    "        # Color image\n",
    "        warped_image = np.zeros((output_shape[0], output_shape[1], image.shape[2]), dtype=image.dtype)\n",
    "    \n",
    "    # Iterate over each pixel in the destination image\n",
    "    for y in range(output_shape[0]):\n",
    "        for x in range(output_shape[1]):\n",
    "            # Apply the inverse homography transformation\n",
    "            dest_point = np.array([x, y, 1])\n",
    "            src_point = H_inv @ dest_point\n",
    "            src_point /= src_point[2]\n",
    "            \n",
    "            src_x, src_y = src_point[0], src_point[1]\n",
    "            \n",
    "            # Check if the source point is within the bounds of the source image\n",
    "            if 0 <= src_x < image.shape[1] and 0 <= src_y < image.shape[0]:\n",
    "                # Bilinear interpolation\n",
    "                x0, y0 = int(np.floor(src_x)), int(np.floor(src_y))\n",
    "                x1, y1 = min(x0 + 1, image.shape[1] - 1), min(y0 + 1, image.shape[0] - 1)\n",
    "                \n",
    "                a = src_x - x0\n",
    "                b = src_y - y0\n",
    "                \n",
    "                if image.ndim == 2:\n",
    "                    # Grayscale image\n",
    "                    pixel_value = (\n",
    "                        (1 - a) * (1 - b) * image[y0, x0] +\n",
    "                        a * (1 - b) * image[y0, x1] +\n",
    "                        (1 - a) * b * image[y1, x0] +\n",
    "                        a * b * image[y1, x1]\n",
    "                    )\n",
    "                    warped_image[y, x] = pixel_value\n",
    "                else:\n",
    "                    # Color image\n",
    "                    for c in range(image.shape[2]):\n",
    "                        pixel_value = (\n",
    "                            (1 - a) * (1 - b) * image[y0, x0, c] +\n",
    "                            a * (1 - b) * image[y0, x1, c] +\n",
    "                            (1 - a) * b * image[y1, x0, c] +\n",
    "                            a * b * image[y1, x1, c]\n",
    "                        )\n",
    "                        warped_image[y, x, c] = pixel_value\n",
    "    \n",
    "    return warped_image\n",
    "\n",
    "def warp_img(image_left, H):\n",
    "    \"\"\"\n",
    "    Warps an image using a given homography matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_left : numpy.ndarray\n",
    "        The input image to be warped.\n",
    "    H : numpy.ndarray\n",
    "        The homography matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        The warped image.\n",
    "    \"\"\"\n",
    "    # Get the height and width of the input image\n",
    "    h_left, w_left = image_left.shape[:2]\n",
    "\n",
    "    # Define the corner points of the input image\n",
    "    C_left = np.array([\n",
    "        [0, 0     , w_left, w_left],\n",
    "        [0, h_left, 0     , h_left],\n",
    "        [1, 1     , 1     , 1     ]\n",
    "    ])\n",
    "\n",
    "    # Apply the homography matrix to the corner points\n",
    "    Cp_left = H @ C_left\n",
    "    Cp_left = Cp_left / Cp_left[-1, :]\n",
    "\n",
    "    # Calculate the minimum x and y coordinates after transformation\n",
    "    w_min, h_min = Cp_left[:-1].min(axis=1).tolist()\n",
    "    w_min, h_min = int(np.abs(w_min)), int(np.abs(h_min))\n",
    "\n",
    "    # Determine the shape of the warped image\n",
    "    warped_image_shape = (w_left + w_min, h_left + h_min)\n",
    "    # Create a translation matrix to shift the image\n",
    "    Ht = np.array([\n",
    "        [1, 0, w_min],\n",
    "        [0, 1, h_min],\n",
    "        [0, 0, 1    ]\n",
    "    ])\n",
    "\n",
    "    # Combine the translation matrix with the homography matrix\n",
    "    Hw = Ht @ H\n",
    "    Hw = Hw / Hw[-1, -1]\n",
    "\n",
    "    # Warp the image using the combined homography matrix\n",
    "    warped_image = warp_perspective_manual(image_left, Hw, warped_image_shape)\n",
    "\n",
    "    return warped_image\n",
    "\n",
    "def create_panorama(img1, img2, H):\n",
    "    \"\"\"\n",
    "    Creates a panorama by stitching two images together using a homography matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    warped_img1 : numpy.ndarray\n",
    "        The first image that has already been warped.\n",
    "    img2 : numpy.ndarray\n",
    "        The second image to be stitched with the first image.\n",
    "    H : numpy.ndarray\n",
    "        The homography matrix that transforms points from the second image to the first image's perspective.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        The resulting panorama image.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the images\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    h2, w2 = img2.shape[:2]\n",
    "\n",
    "    # Get the canvas dimesions\n",
    "    img1_dims = np.float32([[0, 0], [0, h1], [w1, h1], [w1, 0]]).reshape(-1, 1, 2)\n",
    "    img2_dims_temp = np.float32([[0, 0], [0, h2], [w2, h2], [w2, 0]]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Get relative perspective of second image\n",
    "    img2_dims = cv2.perspectiveTransform(img2_dims_temp, H)\n",
    "\n",
    "    # Resulting dimensions\n",
    "    result_dims = np.concatenate((img1_dims, img2_dims), axis = 0)\n",
    "\n",
    "    # Getting images together\n",
    "    # Calculate dimensions of match points\n",
    "    [x_min, y_min] = np.int32(result_dims.min(axis=0).ravel() - 0.5)\n",
    "    [x_max, y_max] = np.int32(result_dims.max(axis=0).ravel() + 0.5)\n",
    "  \n",
    "    # Create output array after affine transformation \n",
    "    transform_dist = [-x_min, -y_min]\n",
    "    transform_array = np.array([[1, 0, transform_dist[0]], \n",
    "                                [0, 1, transform_dist[1]], \n",
    "                                [0,0,1]]) \n",
    "\n",
    "    # Warp images to get the resulting image\n",
    "    result_img = warp_perspective_manual(img1, transform_array.dot(H), (y_max-y_min, x_max-x_min))\n",
    "\n",
    "    # Blend the images where they overlap\n",
    "    overlap_mask = (result_img[transform_dist[1]:h1+transform_dist[1], transform_dist[0]:w1+transform_dist[0]] > 0) & (img2 > 0)\n",
    "    result_img[transform_dist[1]:h1+transform_dist[1], transform_dist[0]:w1+transform_dist[0]][overlap_mask] = (result_img[transform_dist[1]:h1+transform_dist[1], transform_dist[0]:w1+transform_dist[0]][overlap_mask] + img2[overlap_mask]) / 2\n",
    "    result_img[transform_dist[1]:h1+transform_dist[1], transform_dist[0]:w1+transform_dist[0]][~overlap_mask] = img2[~overlap_mask]\n",
    "\n",
    "    # This consolidates the overlap by placing the second image over the first (unused for now)\n",
    "\n",
    "    return result_img\n",
    "\n",
    "def create_panorama_rgb(img1, img2, H):\n",
    "    \"\"\"\n",
    "    Creates a color panorama by stitching two images together using a homography matrix by applying it on each channel.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img1 : numpy.ndarray\n",
    "        The first image to be stitched.\n",
    "    img2 : numpy.ndarray\n",
    "        The second image to be stitched.\n",
    "    H : numpy.ndarray\n",
    "        The homography matrix that transforms points from the second image to the first image's perspective.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        The resulting panorama image.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the images\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    h2, w2 = img2.shape[:2]\n",
    "\n",
    "    # Get the canvas dimesions\n",
    "    img1_dims = np.float32([[0, 0], [0, h1], [w1, h1], [w1, 0]]).reshape(-1, 1, 2)\n",
    "    img2_dims_temp = np.float32([[0, 0], [0, h2], [w2, h2], [w2, 0]]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Get relative perspective of second image\n",
    "    img2_dims = cv2.perspectiveTransform(img2_dims_temp, H)\n",
    "\n",
    "    # Resulting dimensions\n",
    "    result_dims = np.concatenate((img1_dims, img2_dims), axis = 0)\n",
    "\n",
    "    # Getting images together\n",
    "    # Calculate dimensions of match points\n",
    "    [x_min, y_min] = np.int32(result_dims.min(axis=0).ravel() - 0.5)\n",
    "    [x_max, y_max] = np.int32(result_dims.max(axis=0).ravel() + 0.5)\n",
    "  \n",
    "    # Create output array after affine transformation \n",
    "    transform_dist = [-x_min, -y_min]\n",
    "    transform_array = np.array([[1, 0, transform_dist[0]], \n",
    "                                [0, 1, transform_dist[1]], \n",
    "                                [0,0,1]]) \n",
    "\n",
    "    # Warp images to get the resulting image\n",
    "    # result_img = cv2.warpPerspective(img1, transform_array.dot(H), (x_max-x_min, y_max-y_min))\n",
    "    result_img = warp_perspective_manual(img1, transform_array.dot(H), (y_max-y_min, x_max-x_min))\n",
    "    \n",
    "    # Blend the images where they overlap\n",
    "    alpha = 0.5  # Set the weight for the images\n",
    "    overlap_mask = (result_img[transform_dist[1]:h1+transform_dist[1], transform_dist[0]:w1+transform_dist[0]] > 0) & (img2 > 0)\n",
    "    result_img[transform_dist[1]:h1+transform_dist[1], transform_dist[0]:w1+transform_dist[0]][overlap_mask] = alpha * result_img[transform_dist[1]:h1+transform_dist[1], transform_dist[0]:w1+transform_dist[0]][overlap_mask] + (1 - alpha) * img2[overlap_mask]\n",
    "\n",
    "    result_img[transform_dist[1]:h1+transform_dist[1], transform_dist[0]:w1+transform_dist[0]][~overlap_mask] = img2[~overlap_mask]\n",
    "\n",
    "    return result_img\n",
    "\n",
    "\n",
    "# ------------------------------------------------ Main Procesing Function ------------------------------------------------\n",
    "\n",
    "# Function to handle image display\n",
    "def process_images(upload1, upload2):\n",
    "    # Extract images from the uploads\n",
    "    img1_data = upload1.value[0]['content']\n",
    "    img2_data = upload2.value[0]['content']\n",
    "    \n",
    "    if img1_data is None or img2_data is None:\n",
    "        raise ValueError(\"Please upload exactly two images.\")\n",
    "    \n",
    "    # Decode the images using OpenCV\n",
    "    img1_rgb = cv2.imdecode(np.frombuffer(img1_data, np.uint8), cv2.IMREAD_COLOR)\n",
    "    img2_rgb = cv2.imdecode(np.frombuffer(img2_data, np.uint8), cv2.IMREAD_COLOR)\n",
    "\n",
    "    # Convert images to RGB for display\n",
    "    # img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    # img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert the RGB images to PIL Image objects\n",
    "    img1_color = Image.fromarray(img1_rgb)\n",
    "    img2_color = Image.fromarray(img2_rgb)\n",
    "\n",
    "    # Convert images to grayscale\n",
    "    img1_gray = img1_color.convert('L')\n",
    "    img2_gray = img2_color.convert('L')\n",
    "\n",
    "    # Convert grayscale images to numpy arrays\n",
    "    img1 = np.array(img1_gray).astype(float)\n",
    "    img2 = np.array(img2_gray).astype(float)\n",
    "\n",
    "    # plt.imshow(img1, cmap='gray')\n",
    "    cv2.imwrite('./data/out/img1_greyscale.jpg', img1)\n",
    "\n",
    "    # Detect feature points in both images using the Harris corner detector\n",
    "    sigma = 6\n",
    "    thresh = 11\n",
    "    radius = 3\n",
    "    cim1, r1, c1 = harris(img1, sigma, thresh, radius)\n",
    "    cim2, r2, c2 = harris(img2, sigma, thresh, radius)\n",
    "\n",
    "    draw_corners(img1, r1, c1, 'img1')\n",
    "    draw_corners(img2, r2, c2, 'img2')\n",
    "\n",
    "\n",
    "    # Compute SIFT descriptors for the detected feature points\n",
    "\n",
    "    radius_multiplier = 8\n",
    "\n",
    "    radius_1 = np.ones(len(r1)) * radius_multiplier\n",
    "    radius_2 = np.ones(len(r2)) * radius_multiplier\n",
    "\n",
    "    circles_1 = np.ndarray((len(r1), 3))\n",
    "    circles_2 = np.ndarray((len(r2), 3))\n",
    "\n",
    "    for i in range(0, len(r1)):\n",
    "        circles_1[i] = [c1[i], r1[i], radius_1[i]]\n",
    "\n",
    "    for i in range(0, len(r2)):\n",
    "        circles_2[i] = [c2[i], r2[i], radius_2[i]]\n",
    "\n",
    "    enlarge_factor = 1.5\n",
    "\n",
    "    # use find_sift from utils.py\n",
    "    des1 = find_sift(img1, circles_1, enlarge_factor=enlarge_factor)\n",
    "    des2 = find_sift(img2, circles_2, enlarge_factor=enlarge_factor)\n",
    "\n",
    "    des1 = normalize(des1, norm='l2', axis=1)\n",
    "    des2 = normalize(des2, norm='l2', axis=1)\n",
    "\n",
    "    distances = dist2(des1, des2) # Compute distances between the descriptors\n",
    "\n",
    "    # Select Matches based on a distance threshold\n",
    "    thresh = 0.25\n",
    "    filtered_matches = filter_descriptors_by_dist(distances, thresh)\n",
    "    draw_matches(img1, img2, r1, c1, r2, c2, filtered_matches) # Draw matches\n",
    "\n",
    "    # Use RANSAC to find the best homography matrix\n",
    "    # report num of inliers and the average residual for the inliers\n",
    "    H, max_inliers, best_residual, inlier_matches = RANSAC(filtered_matches, 5, 10000, r1, c1, r2, c2)\n",
    "    print(\"number of inliers: \" + str(max_inliers))\n",
    "    print(\"best residual: \" + str(best_residual))\n",
    "    print(\"homography: \")\n",
    "    print(H)\n",
    "\n",
    "    draw_inliers(img1, img2, r1, c1, r2, c2, inlier_matches) # Draw inliers\n",
    "\n",
    "    # Warp the images using the homography matrix\n",
    "    warped_img1 = warp_img(img1, H)\n",
    "    # Save the warped first image\n",
    "    cv2.imwrite('./data/out/warped_img1.jpg', warped_img1)\n",
    "    # Save the second image\n",
    "    cv2.imwrite('./data/out/unwarped_img2.jpg', img2)\n",
    "\n",
    "    # Create a panorama by stitching the images together\n",
    "\n",
    "    # create a panorama in gray scale first\n",
    "    panorama_gray = create_panorama(img1, img2, H)\n",
    "    # Save the grayscale panorama\n",
    "    cv2.imwrite('./data/out/panorama_gray.jpg', panorama_gray)\n",
    "\n",
    "    # Create a color panorama using the homography matrix\n",
    "    panorama = create_panorama_rgb(img1_rgb, img2_rgb, H)\n",
    "    # Save the color panorama\n",
    "    cv2.imwrite('./data/out/panorama_color.jpg', panorama)\n",
    "\n",
    "\n",
    "\n",
    "# Widgets for file upload\n",
    "upload1 = FileUpload(accept='image/*', multiple=False)\n",
    "upload2 = FileUpload(accept='image/*', multiple=False)\n",
    "\n",
    "# Display button\n",
    "display_button = Button(description=\"Stitch Images\")\n",
    "\n",
    "# Output widget for displaying images\n",
    "output = Output()\n",
    "\n",
    "# Link button click to the display_images function\n",
    "display_button.on_click(lambda _: process_images(upload1, upload2))\n",
    "\n",
    "# Custom CSS for styling\n",
    "custom_css = \"\"\"\n",
    "<style>\n",
    "    .widget-upload {\n",
    "        margin: 10px;\n",
    "    }\n",
    "    .widget-button {\n",
    "        background-color: #4CAF50;\n",
    "        color: white;\n",
    "        border: none;\n",
    "        padding-bottom: 10px;\n",
    "        border-radius: 5px;\n",
    "        font-size: 18px;\n",
    "        margin: 10px;\n",
    "        cursor: pointer;\n",
    "    }\n",
    "    .widget-output {\n",
    "        background-color: #f9f9f9;\n",
    "        border-radius: 5px;\n",
    "        margin: 0px;\n",
    "    }\n",
    "    .widget-container {\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "        width: 100%;\n",
    "    }\n",
    "    .widget-row {\n",
    "        display: flex;\n",
    "        justify-content: center;\n",
    "        align-items: center;\n",
    "        width: 100%;\n",
    "    }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "# Display custom CSS\n",
    "display(HTML(custom_css))\n",
    "\n",
    "# Display widgets in the notebook\n",
    "display(VBox([\n",
    "    HBox([upload1, upload2], layout=Layout(justify_content='center', align_items='center')),\n",
    "    display_button,\n",
    "    output\n",
    "], layout=Layout(align_items='center')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "# Homography Image Stitching using Feature Detection and Matching\n",
       "\n",
       "## Overview of the Homography Process\n",
       "\n",
       "The image stitching process undertaken involved the following steps:\n",
       "\n",
       "1. **Loading the Image as Greyscale**: The input images that were uploaded by the user underwent the following preprocessing steps:\n",
       "    - Extraction of the content from the uploaded images (this method is specific to JPG files).\n",
       "    - Decoding the images using ```OpenCV```.\n",
       "    - Conversion to ```PIL``` Image objects, greyscale, and then to numpy ```float``` arrays.\n",
       "2. **Feature Detection**: The Harris corner detector used local intensity gradients to identify corner points in the images. These corner points served as the features for matching.\n",
       "3. **Feature Point Matching**: SIFT descriptors were computed for detected feature points.They were then normalized and pairwise distances between descriptors were calculated for accurate matching.\n",
       "4. **Filtering Matches**:\n",
       "    - Matches were filtered based on a distance threshold.\n",
       "    - RANSAC was used to compute the best homography matrix.\n",
       "    - The homography matric, number of inliers and the average residual for the inliers were reported.\n",
       "5. **Image Warping**: One of the images was warped onto the other using the homography matrix.\n",
       "6. **Image Blending**: The images were blended in Greyscale and then in RGB to create a seamless panorama.\n",
       "\n",
       "## Code Format\n",
       "\n",
       "A set of helper functions were defined to perform the various tasks associated with the image stitching process. The main processing function, ```process_images```, was responsible for orchestrating the entire process. The function took the uploaded images, performed the necessary steps, and generated the final panorama output.\n",
       "The ```widgets``` module was used to create the file upload and button widgets for user interaction. The output was displayed using the ```Output``` widget.\n",
       "         \n",
       "## Step 1: Image loading and Preprocessing\n",
       "         \n",
       "Since the image was uploaded using the ```FileUpload``` widget, the image data was extracted from the uploads and decoded using ```OpenCV```. The images were then converted to greyscale and numpy arrays for further processing.\n",
       "![Greyscale Image 1](./data/out/img1_greyscale.jpg)\n",
       "\n",
       "## Step 2: Feature Detection\n",
       "\n",
       "### Harris Corner Detection\n",
       "\n",
       "The Harris corner detector was used to identify key points in the images. Image gradients are computed in the x and y directions using convolution. \n",
       "         These gradients are smoothed with a Gaussian filter, and the Harris response is calculated to identify corner points. \n",
       "         If the threshold and radius are provided, non-maximal suppression is applied to refine the corner points, returning their coordinates. <br>\n",
       "        The detected corners are shown below:\n",
       "![Detected Corners in Image 1](./data/out/img1_corners.jpg)\n",
       "![Detected Corners in Image 2](./data/out/img2_corners.jpg)\n",
       "\n",
       "### SIFT Descriptors\n",
       "\n",
       "SIFT descriptors for detected feature points are computed by first defining a radius multiplier and creating circles around each feature point. \n",
       "         These circles are then used by the find_sift function from utils.py to compute the SIFT descriptors for both images, with an enlarge factor applied. \n",
       "         The resulting descriptors are normalized using L2 normalization. This ensures that the descriptors are scale-invariant and robust.\n",
       "\n",
       "### Descriptor Filtering\n",
       "The pairwise distances between the descriptors of the two images are calculated. The ones that fall below the specified threshold are considered as potential matches.\n",
       "         ![Feature Matching Output](./data/out/matches.jpg)\n",
       "\n",
       "> **Note**: There generally are some false matches that make it through this filtering process (due to various hyperparameters and the nature of the images). This will be addressed in the next step.\n",
       "\n",
       "## Step 3: Homography Calculation\n",
       "         \n",
       "### `RANSAC` method\n",
       "\n",
       "1. **RANSAC Iterations**:\n",
       "    - For a specified number of iterations, the method randomly selects a subset of matches from the input data.\n",
       "    - The selected matches are used to prepare the corresponding points for transformation computation.\n",
       "\n",
       "2. **Transformation Computation and Prediction**:\n",
       "    - The transformation matrix is computed using the selected points.\n",
       "    - The predicted coordinates are calculated by applying the transformation matrix to the points from the first set.\n",
       "\n",
       "3. **Inlier Detection**:\n",
       "    - The Euclidean distance between the predicted coordinates and the actual coordinates from the second set is computed.\n",
       "    - Points with a distance less than a specified threshold are considered inliers.\n",
       "    - If the number of inliers is greater than the current maximum, the method updates the variables with the new values.\n",
       "\n",
       "4. **Return Best Transformation**:\n",
       "    - After all iterations, the method returns the best transformation matrix, the number of inliers, the best residual error, and the inlier matches.\n",
       "         \n",
       "         \n",
       "### `compute_homography` method\n",
       "\n",
       "1. **Normalization of Points**:\n",
       "    - The method starts by normalizing the input sets of points from two images.\n",
       "    - Normalization involves translating and scaling the points so that their centroid is at the origin and their average distance from the origin is √2.\n",
       "    - This step ensures numerical stability during the computation.\n",
       "\n",
       "2. **Setting Up the Linear System**:\n",
       "    - The method initializes a list to store the coefficients of the linear system.\n",
       "    - For each pair of corresponding points from the two images, two rows are added to the list.\n",
       "    - These rows represent the constraints imposed by the homography transformation, resulting in a system of equations.\n",
       "\n",
       "3. **Solving the System Using SVD**:\n",
       "    - The list of coefficients is converted to a matrix, and Singular Value Decomposition (SVD) is performed on this matrix.\n",
       "    - The homography matrix is obtained from the last row of the right singular vectors, reshaped into a 3x3 matrix.\n",
       "\n",
       "4. **Denormalization and Normalization**:\n",
       "    - The homography matrix is denormalized using the inverse of the normalization matrices obtained earlier.\n",
       "    - The resulting matrix is then normalized so that its bottom-right element is 1, ensuring a consistent scale.\n",
       "    - The final homography matrix is returned.\n",
       "\n",
       "### Visualizing the Inliers\n",
       "The inliers are visualized by drawing lines connecting the matching points in the two images.\n",
       "![Inliers](./data/out/inliers.jpg)\n",
       "         \n",
       "## Step 4: Image Warping\n",
       "         \n",
       "### `warp_img` method\n",
       "The warp_img function warps an input image using a homography matrix by first determining the new positions of the image corners after applying the homography. \n",
       "         It calculates the required size for the warped image to ensure that all transformed corners fit within the new image dimensions. \n",
       "         A translation homography is then applied to adjust the image coordinates, and the final warping is performed using a manual perspective warp function, returning the warped image.\n",
       "\n",
       "### `warp_perspective_manual` method\n",
       "- The function starts by computing the inverse of the homography matrix.\n",
       "- It initializes an output image with the specified shape, ensuring it matches the type (grayscale or color) of the input image.\n",
       "- For each pixel in the output image, the function applies the inverse homography transformation to find the corresponding source pixel in the input image.\n",
       "- If the source pixel is within the bounds of the input image, the function uses bilinear interpolation to compute the pixel value for the output image, ensuring smooth transitions and accurate warping.\n",
       "\n",
       "> **Note**: The warped image contains some black regions as information is lost during the angular and positional transformations. \n",
       "\n",
       "The warped image of the first input image is shown below:\n",
       "![Warped Image 1](./data/out/warped_img1.jpg)\n",
       "         \n",
       "> **Note**: The second image remains unwarped for this part of the process.\n",
       "         \n",
       "## Step 5: Image Stitching (Panorama Creation)\n",
       "         \n",
       "### `create_panorama` Function\n",
       "\n",
       "1. **Initialization**:\n",
       "    - The function starts by obtaining the dimensions of the two input images.\n",
       "    - It defines the corner points of each image to determine their boundaries.\n",
       "\n",
       "2. **Perspective Transformation**:\n",
       "    - The corner points of the second image are transformed using the provided homography matrix to get their relative perspective in the panorama.\n",
       "\n",
       "3. **Canvas Dimensions Calculation**:\n",
       "    - The function calculates the dimensions of the resulting panorama by combining the transformed corner points of both images.\n",
       "    - It determines the minimum and maximum coordinates to create an output canvas that can accommodate both images.\n",
       "\n",
       "4. **Affine Transformation and Warping**:\n",
       "    - An affine transformation matrix is created to adjust the image coordinates, ensuring no negative coordinates.\n",
       "    - The first image is warped onto the output canvas using a manual perspective warp function, applying the combined homography and affine transformation.\n",
       "\n",
       "5. **Blending Overlapping Regions**:\n",
       "    - The function identifies overlapping regions between the two images and blends them by averaging their pixel values.\n",
       "    - It ensures that non-overlapping regions of the second image are directly copied to the output canvas.\n",
       "\n",
       "6. **Return Result**:\n",
       "    - The final panorama image, which seamlessly blends the two input images, is returned.\n",
       "\n",
       "### `create_panorama_rgb` Function\n",
       "\n",
       "1. **Color Image Handling**:\n",
       "    - The function extends the `create_panorama` function to handle color images by processing each color channel separately.\n",
       "    - It ensures that the blending and warping operations are applied consistently across all color channels.\n",
       "\n",
       "2. **Channel-wise Blending**:\n",
       "    - The function blends the overlapping regions for each color channel individually, ensuring smooth transitions and accurate color representation.\n",
       "    - It combines the pixel values of the overlapping regions for each channel and ensures that non-overlapping regions of the second image are directly copied to the output canvas.\n",
       "\n",
       "3. **Final Composition**:\n",
       "    - The function composes the final panorama image by merging the processed color channels.\n",
       "    - It returns the resulting color panorama image, which seamlessly blends the two input images.\n",
       "         \n",
       "## Final Panorama Output\n",
       "         \n",
       "The transition between the two images is seamless, but to highlight the blending of the individual images, their boundaries are preserved.\n",
       "\n",
       "The final grey scale panorama is shown below:\n",
       "![Grey Scale Panorama](./data/out/panorama_gray.jpg)\n",
       "         \n",
       "The final color panorama is shown below:\n",
       "![Color Panorama](./data/out/panorama_color.jpg)\n",
       "  \n",
       "         \n",
       "## Alternative Methods:\n",
       "\n",
       "### Feature Matching\n",
       "\n",
       "Other feature matching methods include:\n",
       "\n",
       "- **FLANN (Fast Library for Approximate Nearest Neighbors)**:\n",
       "    - Uses optimized data structures and algorithms for approximate nearest neighbor search, making it suitable for large datasets.\n",
       "    - **Pros**: Highly efficient for large-scale problems, significantly faster than brute-force methods.\n",
       "    - **Cons**: Slightly more complex to implement and may require parameter tuning for optimal performance.\n",
       "\n",
       "- **K-Nearest Neighbors (KNN) with Lowe's Ratio Test**:\n",
       "    - Finds the k-nearest neighbors for each feature point and applies Lowe's ratio test to filter out false matches by comparing the distance of the closest neighbor to the second closest neighbor.\n",
       "    - **Pros**: Reduces false matches, improves matching accuracy.\n",
       "    - **Cons**: Adds an additional computational step, can be slower for large datasets.\n",
       "\n",
       "- **Brute-Force Matching**:\n",
       "    - Compares each feature descriptor in one image with all feature descriptors in the other image using a distance metric (e.g., Euclidean distance).\n",
       "    - **Pros**: Simple to implement, does not require parameter tuning.\n",
       "    - **Cons**: Computationally expensive, not suitable for large datasets.\n",
       "\n",
       "- **Binary Robust Independent Elementary Features (BRIEF)**:\n",
       "    - Uses binary strings as feature descriptors, which are compared using Hamming distance.\n",
       "    - **Pros**: Fast matching process, low computational cost.\n",
       "    - **Cons**: Less robust to changes in scale and rotation compared to other methods.\n",
       "\n",
       "- **Oriented FAST and Rotated BRIEF (ORB)**:\n",
       "    - Combines the FAST keypoint detector and the BRIEF descriptor with added orientation information to improve robustness.\n",
       "    - **Pros**: Efficient, performs well in real-time applications, free of patent restrictions.\n",
       "    - **Cons**: May not be as accurate as SIFT or SURF, especially in complex scenes.\n",
       "\n",
       "- **Scale-Invariant Feature Transform (SIFT)**:\n",
       "    - Detects keypoints and computes descriptors that are invariant to scale, rotation, and illumination changes.\n",
       "    - **Pros**: Highly distinctive descriptors, robust to various transformations, suitable for a wide range of applications.\n",
       "    - **Cons**: Computationally intensive, slower than other methods, patented.\n",
       "\n",
       "- **Speeded-Up Robust Features (SURF)**:\n",
       "    - Similar to SIFT but uses integral images and a fast Hessian matrix-based detector to speed up the process.\n",
       "    - **Pros**: Faster than SIFT, good balance between speed and accuracy.\n",
       "    - **Cons**: Still more computationally demanding than ORB, patented.\n",
       "\n",
       "- **AKAZE (Accelerated-KAZE)**:\n",
       "    - Uses nonlinear scale spaces for feature detection and description, providing good performance in terms of speed and accuracy.\n",
       "    - **Pros**: Suitable for real-time applications, efficient and accurate.\n",
       "    - **Cons**: May not be as widely supported or tested as SIFT or SURF.\n",
       "\n",
       "### Homography Calculation:\n",
       "\n",
       "- **Direct Linear Transformation (DLT)**:\n",
       "    - Uses a set of linear equations derived from point correspondences to compute the homography matrix.\n",
       "    - **Pros**: Simple to implement, computationally efficient.\n",
       "    - **Cons**: Less robust in the presence of outliers, sensitive to noise.\n",
       "\n",
       "- **Levenberg-Marquardt Optimization**:\n",
       "    - An iterative optimization algorithm that refines the homography matrix by minimizing the reprojection error.\n",
       "    - **Pros**: Provides a more accurate and refined homography matrix, can handle non-linearities.\n",
       "    - **Cons**: Requires an initial estimate, computationally intensive, may converge to local minima.\n",
       "\n",
       "- **Random Sample Consensus (RANSAC)**:\n",
       "    - Iteratively selects random subsets of correspondences to estimate the homography matrix and identifies inliers that fit the model.\n",
       "    - **Pros**: Robust against outliers, does not require an initial estimate.\n",
       "    - **Cons**: Computationally expensive due to multiple iterations, performance depends on the number of iterations and the quality of the data.\n",
       "\n",
       "- **M-estimator Sample Consensus (MSAC)**:\n",
       "    - A variant of RANSAC that uses M-estimators to improve the robustness of the homography estimation.\n",
       "    - **Pros**: More robust to outliers compared to standard RANSAC, can provide better accuracy.\n",
       "    - **Cons**: More complex to implement, computationally intensive.\n",
       "\n",
       "- **Least Median of Squares (LMedS)**:\n",
       "    - Minimizes the median of the squared residuals to estimate the homography matrix.\n",
       "    - **Pros**: Robust to outliers, less sensitive to extreme values.\n",
       "    - **Cons**: Computationally intensive, may not be as accurate as other methods for large datasets.\n",
       "\n",
       "- **Iterative Closest Point (ICP)**:\n",
       "    - Iteratively refines the homography matrix by minimizing the distance between corresponding points.\n",
       "    - **Pros**: Can handle large datasets, provides accurate results.\n",
       "    - **Cons**: Requires good initial alignment, may converge to local minima.\n",
       "         \n",
       "### Image Warping:\n",
       "\n",
       "- **Affine Transformation**: \n",
       "    - Applies linear transformations including rotation, scaling, and translation, but does not handle perspective distortion.\n",
       "    - **Pros**: Simple to implement, efficient for cases involving rotation, scaling, and translation.\n",
       "    - **Cons**: Does not handle perspective distortion, limited to linear transformations.\n",
       "\n",
       "- **Thin Plate Spline (TPS)**: \n",
       "    - Uses a flexible transformation model to handle complex warps and deformations.\n",
       "    - **Pros**: Highly flexible, can handle complex warps and deformations.\n",
       "    - **Cons**: Computationally expensive, more complex to implement.\n",
       "\n",
       "## Summary Table of Methods and Outputs Descriptions\n",
       "\n",
       "| Step                | Method Used          | Output Description                                            |\n",
       "|---------------------|----------------------|----------------------------------------------------------------|\n",
       "| Feature Detection   | Harris Corner Detector | Detected key points in both images.                             |\n",
       "| Feature Matching    | SIFT Descriptors     | Computed and matched SIFT descriptors between the images.      |\n",
       "| Match Filtering     | Distance Threshold | Filtered matches and computed the best homography matrix.       |\n",
       "| Homography Calculation | RANSAC, DLT        | Computed the homography matrix to align the images.             |\n",
       "| Image Warping       | WarpPerspective      | Warped one image to align with the other using homography.      |\n",
       "| Image Blending      | Linear Blending      | Combined the images to create a seamless panorama.              |\n",
       "         \n",
       "## Important Considerations\n",
       "    \n",
       "> **Note**: \n",
       "         - Due to the low-level nature of the implementation, the code may require optimization for large-scale applications.\n",
       "         - Even though the code is efficient for small images, it may not scale well to high-resolution images.\n",
       "         - The ideal resolution for an image is 800x800, as larger images may take longer to process. (with reductions in one dimension and increases in the other)\n",
       "\n",
       "\n",
       "## Conclusion\n",
       "Various feature detection, matching, and homography estimation methods were implemented from scratch, resulting in a highly transparent and customizable image stitching process. \n",
       "The final panorama output demonstrates the effectiveness of the implemented algorithms in aligning and blending images seamlessly. \n",
       "Most of these methods are available in the OpenCV library, but implementing them from scratch provides a deeper understanding of the underlying concepts of image processing and widely used methods in computer vision.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown('''\n",
    "         \n",
    "Here is an implementation of the homography image stitching process using custom-built functions. \n",
    "        \n",
    "<\n",
    "\n",
    "# Homography Image Stitching using Feature Detection and Matching\n",
    "\n",
    "## Overview of the Homography Process\n",
    "\n",
    "The image stitching process undertaken involved the following steps:\n",
    "\n",
    "1. **Loading the Image as Greyscale**: The input images that were uploaded by the user underwent the following preprocessing steps:\n",
    "    - Extraction of the content from the uploaded images (this method is specific to JPG files).\n",
    "    - Decoding the images using ```OpenCV```.\n",
    "    - Conversion to ```PIL``` Image objects, greyscale, and then to numpy ```float``` arrays.\n",
    "2. **Feature Detection**: The Harris corner detector used local intensity gradients to identify corner points in the images. These corner points served as the features for matching.\n",
    "3. **Feature Point Matching**: SIFT descriptors were computed for detected feature points.They were then normalized and pairwise distances between descriptors were calculated for accurate matching.\n",
    "4. **Filtering Matches**:\n",
    "    - Matches were filtered based on a distance threshold.\n",
    "    - RANSAC was used to compute the best homography matrix.\n",
    "    - The homography matric, number of inliers and the average residual for the inliers were reported.\n",
    "5. **Image Warping**: One of the images was warped onto the other using the homography matrix.\n",
    "6. **Image Blending**: The images were blended in Greyscale and then in RGB to create a seamless panorama.\n",
    "\n",
    "## Code Format\n",
    "\n",
    "A set of helper functions were defined to perform the various tasks associated with the image stitching process. The main processing function, ```process_images```, was responsible for orchestrating the entire process. The function took the uploaded images, performed the necessary steps, and generated the final panorama output.\n",
    "The ```widgets``` module was used to create the file upload and button widgets for user interaction. The output was displayed using the ```Output``` widget.\n",
    "         \n",
    "## Step 1: Image loading and Preprocessing\n",
    "         \n",
    "Since the image was uploaded using the ```FileUpload``` widget, the image data was extracted from the uploads and decoded using ```OpenCV```. The images were then converted to greyscale and numpy arrays for further processing.\n",
    "![Greyscale Image 1](./data/out/img1_greyscale.jpg)\n",
    "\n",
    "## Step 2: Feature Detection\n",
    "\n",
    "### Harris Corner Detection\n",
    "\n",
    "The Harris corner detector was used to identify key points in the images. Image gradients are computed in the x and y directions using convolution. \n",
    "         These gradients are smoothed with a Gaussian filter, and the Harris response is calculated to identify corner points. \n",
    "         If the threshold and radius are provided, non-maximal suppression is applied to refine the corner points, returning their coordinates. <br>\n",
    "        The detected corners are shown below:\n",
    "![Detected Corners in Image 1](./data/out/img1_corners.jpg)\n",
    "![Detected Corners in Image 2](./data/out/img2_corners.jpg)\n",
    "\n",
    "### SIFT Descriptors\n",
    "\n",
    "SIFT descriptors for detected feature points are computed by first defining a radius multiplier and creating circles around each feature point. \n",
    "         These circles are then used by the find_sift function from utils.py to compute the SIFT descriptors for both images, with an enlarge factor applied. \n",
    "         The resulting descriptors are normalized using L2 normalization. This ensures that the descriptors are scale-invariant and robust.\n",
    "\n",
    "### Descriptor Filtering\n",
    "The pairwise distances between the descriptors of the two images are calculated. The ones that fall below the specified threshold are considered as potential matches.\n",
    "         ![Feature Matching Output](./data/out/matches.jpg)\n",
    "\n",
    "> **Note**: There generally are some false matches that make it through this filtering process (due to various hyperparameters and the nature of the images). This will be addressed in the next step.\n",
    "\n",
    "## Step 3: Homography Calculation\n",
    "         \n",
    "### `RANSAC` method\n",
    "\n",
    "1. **RANSAC Iterations**:\n",
    "    - For a specified number of iterations, the method randomly selects a subset of matches from the input data.\n",
    "    - The selected matches are used to prepare the corresponding points for transformation computation.\n",
    "\n",
    "2. **Transformation Computation and Prediction**:\n",
    "    - The transformation matrix is computed using the selected points.\n",
    "    - The predicted coordinates are calculated by applying the transformation matrix to the points from the first set.\n",
    "\n",
    "3. **Inlier Detection**:\n",
    "    - The Euclidean distance between the predicted coordinates and the actual coordinates from the second set is computed.\n",
    "    - Points with a distance less than a specified threshold are considered inliers.\n",
    "    - If the number of inliers is greater than the current maximum, the method updates the variables with the new values.\n",
    "\n",
    "4. **Return Best Transformation**:\n",
    "    - After all iterations, the method returns the best transformation matrix, the number of inliers, the best residual error, and the inlier matches.\n",
    "         \n",
    "         \n",
    "### `compute_homography` method\n",
    "\n",
    "1. **Normalization of Points**:\n",
    "    - The method starts by normalizing the input sets of points from two images.\n",
    "    - Normalization involves translating and scaling the points so that their centroid is at the origin and their average distance from the origin is √2.\n",
    "    - This step ensures numerical stability during the computation.\n",
    "\n",
    "2. **Setting Up the Linear System**:\n",
    "    - The method initializes a list to store the coefficients of the linear system.\n",
    "    - For each pair of corresponding points from the two images, two rows are added to the list.\n",
    "    - These rows represent the constraints imposed by the homography transformation, resulting in a system of equations.\n",
    "\n",
    "3. **Solving the System Using SVD**:\n",
    "    - The list of coefficients is converted to a matrix, and Singular Value Decomposition (SVD) is performed on this matrix.\n",
    "    - The homography matrix is obtained from the last row of the right singular vectors, reshaped into a 3x3 matrix.\n",
    "\n",
    "4. **Denormalization and Normalization**:\n",
    "    - The homography matrix is denormalized using the inverse of the normalization matrices obtained earlier.\n",
    "    - The resulting matrix is then normalized so that its bottom-right element is 1, ensuring a consistent scale.\n",
    "    - The final homography matrix is returned.\n",
    "\n",
    "### Visualizing the Inliers\n",
    "The inliers are visualized by drawing lines connecting the matching points in the two images.\n",
    "![Inliers](./data/out/inliers.jpg)\n",
    "         \n",
    "## Step 4: Image Warping\n",
    "         \n",
    "### `warp_img` method\n",
    "The warp_img function warps an input image using a homography matrix by first determining the new positions of the image corners after applying the homography. \n",
    "         It calculates the required size for the warped image to ensure that all transformed corners fit within the new image dimensions. \n",
    "         A translation homography is then applied to adjust the image coordinates, and the final warping is performed using a manual perspective warp function, returning the warped image.\n",
    "\n",
    "### `warp_perspective_manual` method\n",
    "- The function starts by computing the inverse of the homography matrix.\n",
    "- It initializes an output image with the specified shape, ensuring it matches the type (grayscale or color) of the input image.\n",
    "- For each pixel in the output image, the function applies the inverse homography transformation to find the corresponding source pixel in the input image.\n",
    "- If the source pixel is within the bounds of the input image, the function uses bilinear interpolation to compute the pixel value for the output image, ensuring smooth transitions and accurate warping.\n",
    "\n",
    "> **Note**: The warped image contains some black regions as information is lost during the angular and positional transformations. \n",
    "\n",
    "The warped image of the first input image is shown below:\n",
    "![Warped Image 1](./data/out/warped_img1.jpg)\n",
    "         \n",
    "> **Note**: The second image remains unwarped for this part of the process.\n",
    "         \n",
    "## Step 5: Image Stitching (Panorama Creation)\n",
    "         \n",
    "### `create_panorama` Function\n",
    "\n",
    "1. **Initialization**:\n",
    "    - The function starts by obtaining the dimensions of the two input images.\n",
    "    - It defines the corner points of each image to determine their boundaries.\n",
    "\n",
    "2. **Perspective Transformation**:\n",
    "    - The corner points of the second image are transformed using the provided homography matrix to get their relative perspective in the panorama.\n",
    "\n",
    "3. **Canvas Dimensions Calculation**:\n",
    "    - The function calculates the dimensions of the resulting panorama by combining the transformed corner points of both images.\n",
    "    - It determines the minimum and maximum coordinates to create an output canvas that can accommodate both images.\n",
    "\n",
    "4. **Affine Transformation and Warping**:\n",
    "    - An affine transformation matrix is created to adjust the image coordinates, ensuring no negative coordinates.\n",
    "    - The first image is warped onto the output canvas using a manual perspective warp function, applying the combined homography and affine transformation.\n",
    "\n",
    "5. **Blending Overlapping Regions**:\n",
    "    - The function identifies overlapping regions between the two images and blends them by averaging their pixel values.\n",
    "    - It ensures that non-overlapping regions of the second image are directly copied to the output canvas.\n",
    "\n",
    "6. **Return Result**:\n",
    "    - The final panorama image, which seamlessly blends the two input images, is returned.\n",
    "\n",
    "### `create_panorama_rgb` Function\n",
    "\n",
    "1. **Color Image Handling**:\n",
    "    - The function extends the `create_panorama` function to handle color images by processing each color channel separately.\n",
    "    - It ensures that the blending and warping operations are applied consistently across all color channels.\n",
    "\n",
    "2. **Channel-wise Blending**:\n",
    "    - The function blends the overlapping regions for each color channel individually, ensuring smooth transitions and accurate color representation.\n",
    "    - It combines the pixel values of the overlapping regions for each channel and ensures that non-overlapping regions of the second image are directly copied to the output canvas.\n",
    "\n",
    "3. **Final Composition**:\n",
    "    - The function composes the final panorama image by merging the processed color channels.\n",
    "    - It returns the resulting color panorama image, which seamlessly blends the two input images.\n",
    "         \n",
    "## Final Panorama Output\n",
    "         \n",
    "The transition between the two images is seamless, but to highlight the blending of the individual images, their boundaries are preserved.\n",
    "\n",
    "The final grey scale panorama is shown below:\n",
    "![Grey Scale Panorama](./data/out/panorama_gray.jpg)\n",
    "         \n",
    "The final color panorama is shown below:\n",
    "![Color Panorama](./data/out/panorama_color.jpg)\n",
    "  \n",
    "         \n",
    "## Alternative Methods:\n",
    "\n",
    "### Feature Matching\n",
    "\n",
    "Other feature matching methods include:\n",
    "\n",
    "- **FLANN (Fast Library for Approximate Nearest Neighbors)**:\n",
    "    - Uses optimized data structures and algorithms for approximate nearest neighbor search, making it suitable for large datasets.\n",
    "    - **Pros**: Highly efficient for large-scale problems, significantly faster than brute-force methods.\n",
    "    - **Cons**: Slightly more complex to implement and may require parameter tuning for optimal performance.\n",
    "\n",
    "- **K-Nearest Neighbors (KNN) with Lowe's Ratio Test**:\n",
    "    - Finds the k-nearest neighbors for each feature point and applies Lowe's ratio test to filter out false matches by comparing the distance of the closest neighbor to the second closest neighbor.\n",
    "    - **Pros**: Reduces false matches, improves matching accuracy.\n",
    "    - **Cons**: Adds an additional computational step, can be slower for large datasets.\n",
    "\n",
    "- **Brute-Force Matching**:\n",
    "    - Compares each feature descriptor in one image with all feature descriptors in the other image using a distance metric (e.g., Euclidean distance).\n",
    "    - **Pros**: Simple to implement, does not require parameter tuning.\n",
    "    - **Cons**: Computationally expensive, not suitable for large datasets.\n",
    "\n",
    "- **Binary Robust Independent Elementary Features (BRIEF)**:\n",
    "    - Uses binary strings as feature descriptors, which are compared using Hamming distance.\n",
    "    - **Pros**: Fast matching process, low computational cost.\n",
    "    - **Cons**: Less robust to changes in scale and rotation compared to other methods.\n",
    "\n",
    "- **Oriented FAST and Rotated BRIEF (ORB)**:\n",
    "    - Combines the FAST keypoint detector and the BRIEF descriptor with added orientation information to improve robustness.\n",
    "    - **Pros**: Efficient, performs well in real-time applications, free of patent restrictions.\n",
    "    - **Cons**: May not be as accurate as SIFT or SURF, especially in complex scenes.\n",
    "\n",
    "- **Scale-Invariant Feature Transform (SIFT)**:\n",
    "    - Detects keypoints and computes descriptors that are invariant to scale, rotation, and illumination changes.\n",
    "    - **Pros**: Highly distinctive descriptors, robust to various transformations, suitable for a wide range of applications.\n",
    "    - **Cons**: Computationally intensive, slower than other methods, patented.\n",
    "\n",
    "- **Speeded-Up Robust Features (SURF)**:\n",
    "    - Similar to SIFT but uses integral images and a fast Hessian matrix-based detector to speed up the process.\n",
    "    - **Pros**: Faster than SIFT, good balance between speed and accuracy.\n",
    "    - **Cons**: Still more computationally demanding than ORB, patented.\n",
    "\n",
    "- **AKAZE (Accelerated-KAZE)**:\n",
    "    - Uses nonlinear scale spaces for feature detection and description, providing good performance in terms of speed and accuracy.\n",
    "    - **Pros**: Suitable for real-time applications, efficient and accurate.\n",
    "    - **Cons**: May not be as widely supported or tested as SIFT or SURF.\n",
    "\n",
    "### Homography Calculation:\n",
    "\n",
    "- **Direct Linear Transformation (DLT)**:\n",
    "    - Uses a set of linear equations derived from point correspondences to compute the homography matrix.\n",
    "    - **Pros**: Simple to implement, computationally efficient.\n",
    "    - **Cons**: Less robust in the presence of outliers, sensitive to noise.\n",
    "\n",
    "- **Levenberg-Marquardt Optimization**:\n",
    "    - An iterative optimization algorithm that refines the homography matrix by minimizing the reprojection error.\n",
    "    - **Pros**: Provides a more accurate and refined homography matrix, can handle non-linearities.\n",
    "    - **Cons**: Requires an initial estimate, computationally intensive, may converge to local minima.\n",
    "\n",
    "- **Random Sample Consensus (RANSAC)**:\n",
    "    - Iteratively selects random subsets of correspondences to estimate the homography matrix and identifies inliers that fit the model.\n",
    "    - **Pros**: Robust against outliers, does not require an initial estimate.\n",
    "    - **Cons**: Computationally expensive due to multiple iterations, performance depends on the number of iterations and the quality of the data.\n",
    "\n",
    "- **M-estimator Sample Consensus (MSAC)**:\n",
    "    - A variant of RANSAC that uses M-estimators to improve the robustness of the homography estimation.\n",
    "    - **Pros**: More robust to outliers compared to standard RANSAC, can provide better accuracy.\n",
    "    - **Cons**: More complex to implement, computationally intensive.\n",
    "\n",
    "- **Least Median of Squares (LMedS)**:\n",
    "    - Minimizes the median of the squared residuals to estimate the homography matrix.\n",
    "    - **Pros**: Robust to outliers, less sensitive to extreme values.\n",
    "    - **Cons**: Computationally intensive, may not be as accurate as other methods for large datasets.\n",
    "\n",
    "- **Iterative Closest Point (ICP)**:\n",
    "    - Iteratively refines the homography matrix by minimizing the distance between corresponding points.\n",
    "    - **Pros**: Can handle large datasets, provides accurate results.\n",
    "    - **Cons**: Requires good initial alignment, may converge to local minima.\n",
    "         \n",
    "### Image Warping:\n",
    "\n",
    "- **Affine Transformation**: \n",
    "    - Applies linear transformations including rotation, scaling, and translation, but does not handle perspective distortion.\n",
    "    - **Pros**: Simple to implement, efficient for cases involving rotation, scaling, and translation.\n",
    "    - **Cons**: Does not handle perspective distortion, limited to linear transformations.\n",
    "\n",
    "- **Thin Plate Spline (TPS)**: \n",
    "    - Uses a flexible transformation model to handle complex warps and deformations.\n",
    "    - **Pros**: Highly flexible, can handle complex warps and deformations.\n",
    "    - **Cons**: Computationally expensive, more complex to implement.\n",
    "\n",
    "## Summary Table of Methods and Outputs Descriptions\n",
    "\n",
    "| Step                | Method Used          | Output Description                                            |\n",
    "|---------------------|----------------------|----------------------------------------------------------------|\n",
    "| Feature Detection   | Harris Corner Detector | Detected key points in both images.                             |\n",
    "| Feature Matching    | SIFT Descriptors     | Computed and matched SIFT descriptors between the images.      |\n",
    "| Match Filtering     | Distance Threshold | Filtered matches and computed the best homography matrix.       |\n",
    "| Homography Calculation | RANSAC, DLT        | Computed the homography matrix to align the images.             |\n",
    "| Image Warping       | WarpPerspective      | Warped one image to align with the other using homography.      |\n",
    "| Image Blending      | Linear Blending      | Combined the images to create a seamless panorama.              |\n",
    "         \n",
    "## Important Considerations\n",
    "    \n",
    "> **Note**: \n",
    "         - Due to the low-level nature of the implementation, the code may require optimization for large-scale applications.\n",
    "         - Even though the code is efficient for small images, it may not scale well to high-resolution images.\n",
    "         - The ideal resolution for an image is 800x800, as larger images may take longer to process. (with reductions in one dimension and increases in the other)\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "Various feature detection, matching, and homography estimation methods were implemented from scratch, resulting in a highly transparent and customizable image stitching process. \n",
    "The final panorama output demonstrates the effectiveness of the implemented algorithms in aligning and blending images seamlessly. \n",
    "Most of these methods are available in the OpenCV library, but implementing them from scratch provides a deeper understanding of the underlying concepts of image processing and widely used methods in computer vision.\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
